{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a450860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import cudf, glob, gc, pickle\n",
    "\n",
    "VER = 232\n",
    "#PART = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f71e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('../../data/train_data/*_parquet/*')\n",
    "len( files )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eadb35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/Kaggle/otto/valid/test_parquet/004.parquet',\n",
       " '/raid/Kaggle/otto/valid/test_parquet/019.parquet',\n",
       " '/raid/Kaggle/otto/valid/test_parquet/008.parquet',\n",
       " '/raid/Kaggle/otto/valid/test_parquet/011.parquet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a4ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_weight = {0:1, 1:1, 2:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83770d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(f):\n",
    "    df = cudf.read_parquet(f)\n",
    "    #df['d'] = cudf.to_datetime(df.ts*1e9).dt.day.astype('int8')\n",
    "    #df.ts = (df.ts/1000).astype('int32')\n",
    "    #df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8be254ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### PART 1\n",
      "Processing 20 files...\n",
      "0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , \n",
      "Processing 20 files...\n",
      "20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , \n",
      "Processing 20 files...\n",
      "40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , \n",
      "Processing 20 files...\n",
      "60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 , \n",
      "Processing 20 files...\n",
      "80 , 81 , 82 , 83 , 84 , 85 , 86 , 87 , 88 , 89 , 90 , 91 , 92 , 93 , 94 , 95 , 96 , 97 , 98 , 99 , \n",
      "Processing 20 files...\n",
      "100 , 101 , 102 , 103 , 104 , 105 , 106 , 107 , 108 , 109 , 110 , 111 , 112 , 113 , 114 , 115 , 116 , 117 , 118 , 119 , \n",
      "CPU times: user 1min 28s, sys: 42.7 s, total: 2min 11s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PIECES = 1\n",
    "SIZE = 1.86e6/PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(PIECES):\n",
    "    print()\n",
    "    print('### PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for a,b in [(0,20),(20,40),(40,60),(60,80),(80,100),(100,120)]:\n",
    "        print(f'Processing {b-a} files...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        READ_CT = 1\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,True])\n",
    "            df['k'] = np.arange(len(df))\n",
    "            # USE TAIL OF SESSION\n",
    "            #df = df.reset_index(drop=True)\n",
    "            #df['n'] = df.groupby('session').cumcount()\n",
    "            #df = df.loc[df.n<100].drop('n',axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df.drop_duplicates(['session','aid']),on=['session'])\n",
    "            df = df.loc[ ((df.k_y - df.k_x).abs()>=1) & ((df.k_y - df.k_x).abs()<=3) & (df.aid_x != df.aid_y) ]\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            #df = df.sort_values('ts_x',ascending=False)\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','ts_x','ts_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            w = (1/2)**( (df.ts_x - df.ts_y).abs() /60/60)\n",
    "            df['wgt'] = w #df.type_y.map(type_weight)            \n",
    "            #df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "            \n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<80].drop('n',axis=1)\n",
    "    # SAVE PART TO DISK\n",
    "    df = tmp.to_pandas().groupby('aid_x').aid_y.apply(list)\n",
    "    with open(f'../../data/covisit_matrices/top_40_aids_v{VER}_{PART}.pkl', 'wb') as f:\n",
    "        pickle.dump(df.to_dict(), f)\n",
    "        \n",
    "# Wall time: 1min 48s\n",
    "# Wall time: 1min 39s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
